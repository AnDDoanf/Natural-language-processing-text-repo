{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnDDoanf/learn_NLP/blob/master/notebooks/CRF_Models_for_Vietnamese_Word_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmYbIgwa3Jgi"
      },
      "source": [
        "# Using Conditional Random Fields for Vietnamese Word Segmentation\n",
        "\n",
        "In this notebook, we will show a solution for the programming assignment 4 - Vietnamese Word Segmentation. We are using Conditional Random Fields (CRFs) model for the task.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMtZxsMi3dId"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "You will use the training data in the file [train.txt](https://www.dl.dropboxusercontent.com/s/reor8jnqedk7svt/train.txt) to train your Vietnamese Word Segmentation Model and evaluate the model on the test data in the file [test.txt](https://www.dl.dropboxusercontent.com/s/zp635cd1zhofm62/test.txt) derived from VLSP 2013 Word Segmentation dataset.\n",
        "\n",
        "The training data contains 20000 sentences (sentences are separated by a blank line), and the test data contains 2000 sentences.\n",
        "\n",
        "You can download the file using wget command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNZxOWmzcGAO"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!rm -f train.txt\n",
        "!wget https://www.dl.dropboxusercontent.com/s/reor8jnqedk7svt/train.txt\n",
        "\n",
        "!rm -f test.txt\n",
        "!wget https://www.dl.dropboxusercontent.com/s/zp635cd1zhofm62/test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toox60Fb3eZv"
      },
      "source": [
        "## Install necessary packages\n",
        "\n",
        "We will use following packages:\n",
        "\n",
        "- [python-crfsuite](https://github.com/scrapinghub/python-crfsuite) is a python binding to CRFsuite.\n",
        "- seqeval for sequence tagging evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3mrjGqXclPy"
      },
      "source": [
        "%%capture\n",
        "!pip install -q seqeval[cpu]\n",
        "!pip install -q python-crfsuite"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWRDZUJHc35C"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "We will load data into a list of tuples (word, tag) by using the following function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajfHNumjc8gY"
      },
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"Load data from a file (train.txt or test.txt)\n",
        "\n",
        "    Return:\n",
        "        tagged_sentences (list): List of sentence. Each sentence is a list of tuples (word, tag)\n",
        "    \"\"\"\n",
        "    # TODO: Write your code here\n",
        "    tagged_sentences = []\n",
        "    cur_sen = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            line = line.rstrip()\n",
        "            if line == '':\n",
        "                if len(cur_sen) != 0:\n",
        "                    tagged_sentences.append(cur_sen)\n",
        "                    cur_sen = []\n",
        "            else:\n",
        "                word, tag = line.split()\n",
        "                cur_sen.append((word, tag))\n",
        "    if len(cur_sen) != 0:\n",
        "        tagged_sentences.append(cur_sen)\n",
        "    return tagged_sentences\n",
        "\n",
        "train_data = load_data('train.txt')\n",
        "test_data = load_data('test.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdZsKFeldBBQ"
      },
      "source": [
        "## Features\n",
        "\n",
        "In this section, we are going to implement features in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VB87g5rdOx_"
      },
      "source": [
        "def word2features(sentence, i):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        sentence (list): list of words [w1, w2,...,w_n]\n",
        "        i (int): index of the word\n",
        "    Return:\n",
        "        features (dict): dictionary of features\n",
        "    \"\"\"\n",
        "    word = sentence[i]\n",
        "    features = {\n",
        "        'is_first': i == 0,\n",
        "        'is_last': i == len(sentence) - 1,\n",
        "        'is_first_capital': word[0].isupper(),\n",
        "        'is_all_caps': int(word.upper() == word),\n",
        "        'is_all_lower': word.lower() == word,\n",
        "        'word': word,\n",
        "        'word.lower()': word.lower(),\n",
        "        'prefix_1': word[0],\n",
        "        'prefix_2': word[:2],\n",
        "        'prefix_3': word[:3],\n",
        "        'prefix_4': word[:4],\n",
        "        'suffix_1': word[-1],\n",
        "        'suffix_2': word[-2:],\n",
        "        'suffix_3': word[-3:],\n",
        "        'suffix_4': word[-4:],\n",
        "        'has_hyphen': '-' in word,\n",
        "        'is_numeric': word.isdigit(),\n",
        "        'capitals_inside': word[1:].lower() != word[1:],\n",
        "        # word unigram, bigram, and trigram\n",
        "        'word[i-2].lower()': '' if i-2<0 else sentence[i-2].lower(),\n",
        "        'word[i-1].lower()': '' if i-1<0 else sentence[i-1].lower(),\n",
        "        'word[i+1].lower()': '' if i+1>=len(sentence) else sentence[i+1].lower(),\n",
        "        'word[i+2].lower()': '' if i+2>=len(sentence) else sentence[i+2].lower(),\n",
        "\n",
        "        'word[i-2]': '' if i-2<0 else sentence[i-2],\n",
        "        'word[i-1]': '' if i-1<0 else sentence[i-1],\n",
        "        'word[i+1]': '' if i+1>=len(sentence) else sentence[i+1],\n",
        "        'word[i+2]': '' if i+2>=len(sentence) else sentence[i+2],\n",
        "\n",
        "        'words[-2,-1]': '' if i-2 < 0 else ' '.join(sentence[i-2:i]),\n",
        "        'words[-1,0]': '' if i-1 < 0 else ' '.join(sentence[i-1:i+1]),\n",
        "        'words[0,1]': '' if i+1>=len(sentence) else ' '.join(sentence[i:i+2]),\n",
        "        'words[1,2]': '' if i+2>=len(sentence) else ' '.join(sentence[i+1:i+3]),\n",
        "        'words[-2,0]': '' if i-2<0 else ' '.join(sentence[i-2:i+1]),\n",
        "        'words[-1,1]': '' if i-1<0 or i+1>=len(sentence) else ' '.join(sentence[i-1:i+1]),\n",
        "        'words[0,2]': '' if i+2>=len(sentence) else ' '.join(sentence[i:i+3]),                                                 \n",
        "    }\n",
        "    \n",
        "    return features\n",
        "\n",
        "\n",
        "def sent2features(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of words [w1, w2,...,w_n]\n",
        "    \"\"\"\n",
        "    return [word2features(sentence, i) for i in range(len(sentence))]\n",
        "\n",
        "\n",
        "def sent2labels(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"    \n",
        "    return [postag for token, postag in sentence]\n",
        "\n",
        "def untag(sentence):\n",
        "    \"\"\"\n",
        "    sentence is a list of tuples (word, postag)\n",
        "    \"\"\"\n",
        "    return [token for token, _ in sentence]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL60-uSzih9k"
      },
      "source": [
        "Let's see how the feature function works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMfnrRErikTm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180be73d-1898-48c2-ad32-e824eee5ee17"
      },
      "source": [
        "sent2features(untag(train_data[0]))[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'is_first': True,\n",
              " 'is_last': False,\n",
              " 'is_first_capital': True,\n",
              " 'is_all_caps': 0,\n",
              " 'is_all_lower': False,\n",
              " 'word': 'Nam',\n",
              " 'word.lower()': 'nam',\n",
              " 'prefix_1': 'N',\n",
              " 'prefix_2': 'Na',\n",
              " 'prefix_3': 'Nam',\n",
              " 'prefix_4': 'Nam',\n",
              " 'suffix_1': 'm',\n",
              " 'suffix_2': 'am',\n",
              " 'suffix_3': 'Nam',\n",
              " 'suffix_4': 'Nam',\n",
              " 'has_hyphen': False,\n",
              " 'is_numeric': False,\n",
              " 'capitals_inside': False,\n",
              " 'word[i-2].lower()': '',\n",
              " 'word[i-1].lower()': '',\n",
              " 'word[i+1].lower()': 'hồn',\n",
              " 'word[i+2].lower()': 'nhiên',\n",
              " 'word[i-2]': '',\n",
              " 'word[i-1]': '',\n",
              " 'word[i+1]': 'hồn',\n",
              " 'word[i+2]': 'nhiên',\n",
              " 'words[-2,-1]': '',\n",
              " 'words[-1,0]': '',\n",
              " 'words[0,1]': 'Nam hồn',\n",
              " 'words[1,2]': 'hồn nhiên',\n",
              " 'words[-2,0]': '',\n",
              " 'words[-1,1]': '',\n",
              " 'words[0,2]': 'Nam hồn nhiên'}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL0D7dZDimmv"
      },
      "source": [
        "Now we can extract features from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM9WhrIxit6x"
      },
      "source": [
        "X_train = [sent2features(untag(s)) for s in train_data]\n",
        "y_train = [sent2labels(s) for s in train_data]\n",
        "\n",
        "X_test = [sent2features(untag(s)) for s in test_data]\n",
        "y_test = [sent2labels(s) for s in test_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyAs2zR4ivx_"
      },
      "source": [
        "## Training\n",
        "\n",
        "To see all possible CRF parameters check its docstring. Here we are using SGD training algorithm with L2 regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C6XfHLaiyjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb51cdd-a1f3-4ffc-bc49-949b2daeb53f"
      },
      "source": [
        "%%time\n",
        "import pycrfsuite\n",
        "\n",
        "trainer = pycrfsuite.Trainer(algorithm='lbfgs', verbose=False)\n",
        "\n",
        "for xseq, yseq in zip(X_train, y_train):\n",
        "    trainer.append(xseq, yseq)\n",
        "\n",
        "trainer.set_params({\n",
        "    'c1': 1.0,   # coefficient for L1 penalty\n",
        "    'c2': 1e-3,  # coefficient for L2 penalty\n",
        "    'max_iterations': 50,  # stop earlier\n",
        "\n",
        "    # include transitions that are possible, but not observed\n",
        "    'feature.possible_transitions': True\n",
        "})\n",
        "\n",
        "trainer.train('word_segmenter.crfsuite')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2min 13s, sys: 956 ms, total: 2min 14s\n",
            "Wall time: 2min 15s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RmcHmHsi3L8"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Evaluation measures:\n",
        "\n",
        "- P(recision): (Number of word models correctly split)/(Number of words in the model's output)\n",
        "- R(ecall): (Number of word models correctly split)/(Number of words in ground-truth data)\n",
        "- F1 = 2*P*R/(P+R)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first load model to a tagger and then predict on the test data"
      ],
      "metadata": {
        "id": "dm-hIj90ErEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import f1_score, classification_report\n",
        "\n",
        "tagger = pycrfsuite.Tagger()\n",
        "tagger.open('word_segmenter.crfsuite')\n",
        "predicted_tag_sequences =[tagger.tag(xseq) for xseq in X_test]\n",
        "\n",
        "print(classification_report(y_test, predicted_tag_sequences))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLtWOea-Ewc3",
        "outputId": "55a5adda-fb17-4298-a6c2-cf8d62ea0f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           W       0.96      0.97      0.96     62131\n",
            "\n",
            "   micro avg       0.96      0.97      0.96     62131\n",
            "   macro avg       0.96      0.97      0.96     62131\n",
            "weighted avg       0.96      0.97      0.96     62131\n",
            "\n"
          ]
        }
      ]
    }
  ]
}